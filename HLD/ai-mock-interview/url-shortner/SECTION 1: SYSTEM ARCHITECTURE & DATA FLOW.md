# **SECTION 1: SYSTEM ARCHITECTURE & DATA FLOW** ğŸ—ï¸

---

## **1.1 Cache Strategy & Placement**

### ğŸ¤” Interview Question/Context

**Interviewer:** "You mentioned caching in your architecture diagram. Walk me through exactly where the cache sits and how it's used. What happens when a user clicks a short URL?"

### âŒ Your Initial Answer

You drew a cache component in your diagram connected to the service layer, but when asked about the **read flow**, you didn't clearly explain:
- Whether you check cache before or after decoding the short URL
- What exactly gets cached (short URL? ID? both?)
- Cache miss behavior
- When data gets written to cache

Your diagram showed: `Service â†’ Cache â†’ DB` but the **data flow wasn't crystal clear**.

---

### âœ… Complete Answer

#### **What is Caching?**

Caching is storing frequently accessed data in **fast storage (memory)** to avoid hitting **slow storage (database)** repeatedly.

**Key principle:** 
- Cache reads: ~1ms (Redis in-memory)
- Database reads: ~10-50ms (disk I/O)
- **50x speed improvement!**

---

#### **Two Types of Caching in URL Shortener:**

### **A) Read Cache (Hot URL Cache)** - PRIMARY USE CASE

**Purpose:** Speed up redirects (GET /{shortCode})

**What to cache:**
```
Key: "url:123456789" (ID as key)
Value: "https://google.com" (long URL as value)
TTL: 1 hour (or never expire for permanent URLs)
```

**Why ID as key, not short code?**
- You decode `aBc123` â†’ ID `123456789` anyway
- Integer keys are slightly more efficient
- Consistent with database primary key

---

### **B) Write Cache (Duplicate Detection Cache)** - SECONDARY USE CASE

**Purpose:** Fast duplicate checking when creating URLs

**What to cache:**
```
Key: "longurl:https://google.com" (long URL as key)
Value: "123456789" (ID as value)
TTL: 24 hours
```

**Why?**
- Avoid DB query for "does this URL already exist?"
- 95% of duplicate checks served from cache

---

### ğŸ¯ URL Shortener Context

#### **Architecture Placement:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     REDIRECT FLOW (Read)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User clicks: https://tiny.url/aBc123
        â†“
[Load Balancer]
        â†“
[Service Instance]
        â†“
   Decode: aBc123 â†’ ID: 123456789
        â†“
        â†“
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  CHECK REDIS CACHE                   â”‚
   â”‚  Key: "url:123456789"                â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€ Cache HIT (95% of requests) â”€â”€â†’ Return "https://google.com"
        â”‚                                     â†“
        â”‚                                  [302 Redirect]
        â”‚                                     â†“
        â”‚                                  Success! (1-2ms latency)
        â”‚
        â”‚
        â””â”€â”€â”€ Cache MISS (5% of requests)
                    â†“
            [Query Slave DB]
            SELECT long_url FROM url_mappings WHERE id = 123456789
                    â†“
            Got: "https://google.com"
                    â†“
            [Write to Redis Cache] â† IMPORTANT: Update cache!
            SET "url:123456789" = "https://google.com" EX 3600
                    â†“
            Return "https://google.com"
                    â†“
            [302 Redirect]
                    â†“
            Success! (10-20ms latency)
```

---

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CREATE URL FLOW (Write)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User: POST /api/v1/urls {"longUrl": "https://google.com"}
        â†“
[Load Balancer]
        â†“
[Service Instance]
        â†“
        â†“
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  CHECK DUPLICATE IN CACHE (Optional)      â”‚
   â”‚  Key: "longurl:https://google.com"        â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€ Cache HIT â”€â”€â†’ Return existing short URL
        â”‚                  (No DB write needed!)
        â”‚
        â””â”€â”€â”€ Cache MISS
                    â†“
            [Check Master DB]
            SELECT id FROM url_mappings 
            WHERE long_url = 'https://google.com'
                    â†“
                    â”œâ”€â”€â”€ Exists â”€â”€â†’ Return existing
                    â”‚
                    â””â”€â”€â”€ Not exists
                            â†“
                    Generate Snowflake ID: 123456789
                            â†“
                    [Write to Master DB]
                    INSERT INTO url_mappings (id, long_url) 
                    VALUES (123456789, 'https://google.com')
                            â†“
                    [Write to Redis Cache] â† IMPORTANT!
                    SET "url:123456789" = "https://google.com"
                    SET "longurl:https://google.com" = "123456789"
                            â†“
                    Return: {"shortUrl": "https://tiny.url/aBc123"}
```

---

#### **Cache Patterns: Cache-Aside vs Write-Through**

**Cache-Aside Pattern (Lazy Loading)** âœ… RECOMMENDED FOR URL SHORTENER

```
Read:
1. Check cache
2. If MISS â†’ Query DB â†’ Write to cache â†’ Return
3. If HIT â†’ Return directly

Write:
1. Write to DB
2. Invalidate/update cache (optional)
```

**Why this is best:**
- âœ… Only hot data ends up in cache (self-optimizing)
- âœ… Cache failures don't block reads (degrade to DB)
- âœ… Simple to implement

---

**Write-Through Pattern** âŒ NOT RECOMMENDED HERE

```
Write:
1. Write to cache
2. Write to DB synchronously

Read:
1. Always check cache first
```

**Why NOT for URL shortener:**
- âŒ Every created URL goes to cache (even if never clicked)
- âŒ Cache becomes bloated with cold data
- âŒ Cache failure blocks writes

---

#### **Cache Sizing Calculation:**

**Question:** How big should the Redis cache be?

**Analysis:**
- Total URLs: 24 billion (over 20 years)
- Hot URLs: Top 10 million most-clicked (0.04% of total)
- Average entry size:
  - Key: `url:123456789` = 15 bytes
  - Value: `https://example.com/path` = 100 bytes average
  - Total per entry: ~115 bytes

**Calculation:**
```
10 million entries Ã— 115 bytes = 1.15 GB
Add 30% overhead (Redis metadata) = 1.5 GB
Add buffer for write cache = 2 GB total
```

**Cache configuration:**
```
Redis:
  Max memory: 4 GB (room for growth)
  Eviction policy: LRU (Least Recently Used)
  Persistence: RDB snapshots (for recovery)
```

---

#### **Cache Hit Rate Optimization:**

**Target: 95%+ hit rate**

**Strategies:**

**1. TTL Strategy:**
```
Hot URLs (>100 clicks/day): TTL = Never expire
Warm URLs (10-100 clicks/day): TTL = 1 hour
Cold URLs: Don't cache
```

**2. Prewarming:**
```
On service startup:
  - Load top 1000 URLs into cache
  - Prevents "thundering herd" at cold start
```

**3. Monitoring:**
```
Track metrics:
  - cache_hit_rate = hits / (hits + misses)
  - cache_memory_usage
  - cache_eviction_rate
  
Alert if:
  - Hit rate < 90% (cache too small or TTL too short)
  - Eviction rate > 1000/sec (cache too small)
```

---

#### **Handling Cache Failures:**

**Scenario:** Redis cluster goes down

**Without graceful degradation:**
```
User clicks URL â†’ Service checks cache â†’ TIMEOUT (Redis down)
â†’ Service crashes or hangs â†’ ALL requests fail âŒ
```

**With graceful degradation:**
```
try:
    long_url = redis.get(f"url:{id}", timeout=0.1)  # 100ms timeout
    if long_url:
        return long_url
except RedisTimeout:
    log.warning("Redis timeout, falling back to DB")
except RedisError:
    log.error("Redis error, falling back to DB")

# Fallback: Query database directly
long_url = db.query("SELECT long_url FROM url_mappings WHERE id = ?", id)
return long_url
```

**Result:**
- âœ… Service stays available (slower, but works)
- âœ… No cascading failure
- âœ… Alerts fire for ops team to fix Redis

---

### ğŸ“Š Key Takeaways

1. **Cache placement:** Between service and database, check cache BEFORE hitting DB

2. **Two caches:** 
   - Read cache (url:ID â†’ long_url) - 95% hit rate target
   - Write cache (longurl:URL â†’ ID) - Optional, for duplicate detection

3. **Cache-aside pattern:** Best for read-heavy workloads like URL shortener

4. **Size conservatively:** 2-4 GB Redis handles 10 million hot URLs

5. **Graceful degradation:** Always fallback to DB if cache fails

6. **Monitor religiously:** Track hit rate, evictions, memory usage

---

### ğŸ”— Further Reading

- **Redis documentation:** Eviction policies (LRU, LFU, TTL)
- **Caching strategies:** "Cache-Aside vs Write-Through vs Write-Behind"
- **Real-world example:** Twitter's "Manhattan" distributed cache

---

### âœï¸ Practice Exercise

**Scenario:** Your cache hit rate drops from 95% to 60% suddenly.

**Question:** What could be the causes? How would you investigate?

<details>
<summary>Click to see answer</summary>

**Possible causes:**
1. **Traffic spike with new URLs** - Many new URLs being clicked (cache hasn't warmed up yet)
2. **Cache size too small** - LRU evicting hot URLs too quickly
3. **TTL too short** - Hot URLs expiring before next access
4. **Uneven traffic distribution** - Viral link causing one URL to dominate cache space

**Investigation steps:**
1. Check Grafana: Plot cache hit rate vs traffic volume
2. Check Redis: `INFO stats` - evicted_keys, expired_keys
3. Check application logs: Which URLs are missing from cache?
4. Query analytics DB: Top 100 most-clicked URLs in last hour

**Solutions:**
- Increase cache size (scale up Redis)
- Increase TTL for hot URLs
- Pre-warm cache with known viral URLs
- Add a second cache tier (L1 = hot, L2 = warm)

</details>

---

## **1.2 Rate Limiter Architecture**

### ğŸ¤” Interview Question/Context

**Interviewer:** "I see you placed rate limiting after the load balancer. Walk me through what happens when the same user makes requests to different service instances. Does your rate limiting work correctly?"

### âŒ Your Initial Answer

In your architecture diagram, you showed:

```
Client â†’ Load Balancer â†’ Rate Limiter â†’ Service Instances
```

The issue: You placed rate limiting as a **separate component** but didn't explain:
- How rate limiter state is shared across services
- What happens if requests go to different servers
- Where the rate limit counters are stored

**The critical mistake:** If each service has its own rate limiter, a user can bypass limits by hitting different servers!

---

### âœ… Complete Answer

#### **The Problem with Distributed Rate Limiting:**

**Scenario without centralized state:**

```
User makes 15 requests in 1 second (limit is 10/sec)

Load Balancer distributes:
  Request 1-5  â†’ Service A (checks local counter: 5 requests âœ…)
  Request 6-10 â†’ Service B (checks local counter: 5 requests âœ…)
  Request 11-15â†’ Service C (checks local counter: 5 requests âœ…)

Result: User made 15 requests, all allowed! âŒ
Rate limit bypassed!
```

**Root cause:** Each service instance has **independent state** - they don't know about requests handled by other instances.

---

#### **Solution: Centralized Rate Limiting with Redis**

**Architecture:**

```
                    [Client]
                       â†“
               [Load Balancer]
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“              â†“              â†“
   [Service A]   [Service B]   [Service C]
        â†“              â†“              â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
                  [Redis Cluster]
              (Shared rate limit state)
```

**How it works:**

```
Every service instance:
1. Receives request
2. Asks Redis: "Has user X exceeded limit?"
3. Redis responds: Yes/No (atomic operation)
4. Service allows or rejects request
```

**Redis stores:**
```
Key: "ratelimit:user:12345:create_url"
Value: 7 (number of requests in current window)
TTL: 60 seconds (sliding window)
```

---

#### **Token Bucket Implementation in Distributed System:**

**Redis data structure:**

```
Key: "ratelimit:user:12345"
Value: JSON {
  "tokens": 3,              // Remaining tokens
  "capacity": 10,           // Max tokens
  "refill_rate": 1,         // Tokens per second
  "last_refill": 1638360000 // Unix timestamp
}
TTL: 3600 seconds (1 hour)
```

**Service logic (pseudocode for HLD):**

```
FUNCTION check_rate_limit(user_id, action):
    key = "ratelimit:" + user_id + ":" + action
    
    // Atomic operation in Redis (Lua script)
    result = REDIS.EVAL("""
        -- Get current bucket
        local bucket = redis.call('GET', KEYS[1])
        if not bucket then
            -- First request - initialize bucket
            bucket = {tokens: CAPACITY, last_refill: now()}
        end
        
        -- Calculate tokens to add based on time elapsed
        elapsed = now() - bucket.last_refill
        tokens_to_add = elapsed * REFILL_RATE
        bucket.tokens = min(CAPACITY, bucket.tokens + tokens_to_add)
        bucket.last_refill = now()
        
        -- Check if request allowed
        if bucket.tokens >= 1 then
            bucket.tokens = bucket.tokens - 1
            redis.call('SETEX', KEYS[1], TTL, bucket)
            return {allowed: true, remaining: bucket.tokens}
        else
            return {allowed: false, retry_after: 1/REFILL_RATE}
        end
    """, key)
    
    RETURN result
```

**Why Lua script?**
- âœ… **Atomic operation** - no race conditions
- âœ… **Single round-trip** to Redis (not multiple GET/SET calls)
- âœ… **Guaranteed consistency** across all service instances

---

### ğŸ¯ URL Shortener Context

#### **Rate Limiting Strategy:**

**Different limits for different actions:**

```
Action: Create URL
  Unauthenticated user (by IP): 10 URLs/day
  Authenticated free user: 100 URLs/day
  Authenticated premium user: 10,000 URLs/day

Action: Access URL (redirect)
  Any user: 1,000 requests/minute (DDoS protection)
  
Action: Custom URL creation
  Any authenticated user: 5 custom URLs/day
```

**Identifier strategies:**

```
1. IP Address:
   Key: "ratelimit:ip:192.168.1.1:create_url"
   Problem: Shared IPs (NAT), VPNs bypass
   Use case: Unauthenticated users

2. User ID:
   Key: "ratelimit:user:12345:create_url"
   Problem: Requires authentication
   Use case: Authenticated users

3. API Key:
   Key: "ratelimit:apikey:abc123:create_url"
   Problem: API key can be stolen
   Use case: Programmatic access

BEST PRACTICE: Combine multiple identifiers
```

---

#### **Layered Rate Limiting:**

**Defense in depth approach:**

```
Layer 1: CDN/DDoS Protection (Cloudflare)
  â””â”€ 10,000 requests/sec per IP (blocks DDoS attacks)
         â†“
Layer 2: Load Balancer (NGINX)
  â””â”€ 1,000 requests/sec per IP (prevents overload)
         â†“
Layer 3: Application Rate Limiter (Redis)
  â””â”€ 10 URLs/day per user (business logic)
         â†“
Layer 4: Database Rate Limiter
  â””â”€ Max 500 connections (prevents DB overload)
```

**Each layer protects the next!**

---

#### **Where Rate Limiting Happens in Request Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CREATE URL WITH RATE LIMITING               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

POST /api/v1/urls {"longUrl": "..."}
        â†“
[Load Balancer]
  - Layer 2 check: IP not exceeding 1000 req/sec? âœ“
        â†“
[Service Instance A]
        â†“
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  RATE LIMIT CHECK (FIRST THING!)            â”‚
   â”‚                                              â”‚
   â”‚  Identify user:                              â”‚
   â”‚    - Authenticated? Use user_id              â”‚
   â”‚    - No? Use IP address                      â”‚
   â”‚                                              â”‚
   â”‚  Query Redis:                                â”‚
   â”‚  "ratelimit:user:12345:create_url"           â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€ ALLOWED â”€â”€â†’ Continue processing
        â”‚                    â†“
        â”‚                Validate URL
        â”‚                    â†“
        â”‚                Check duplicate
        â”‚                    â†“
        â”‚                Generate ID
        â”‚                    â†“
        â”‚                Save to DB
        â”‚                    â†“
        â”‚                Return 201
        â”‚
        â”‚
        â””â”€â”€â”€ REJECTED â”€â”€â†’ Return 429 Too Many Requests
                          {
                            "error": "RATE_LIMIT_EXCEEDED",
                            "message": "You can create 10 URLs per day",
                            "retry_after": 3600
                          }
                          
                          Headers:
                            X-RateLimit-Limit: 10
                            X-RateLimit-Remaining: 0
                            X-RateLimit-Reset: 1638363600
                            Retry-After: 3600
```

**Key principle:** Rate limit check happens **BEFORE** any expensive operations (DB queries, external API calls, etc.)

---

#### **Handling Redis Failures (Rate Limiter Down):**

**Scenario:** Redis cluster goes down - what happens to rate limiting?

**Option 1: Fail Closed (Deny all requests)** âŒ
```
Redis down â†’ Can't check rate limit â†’ Reject all requests â†’ Service unusable
```

**Option 2: Fail Open (Allow all requests)** âš ï¸
```
Redis down â†’ Can't check rate limit â†’ Allow all requests â†’ Vulnerable to abuse
```

**Option 3: Graceful Degradation (Smart fallback)** âœ…
```
try:
    rate_limit_check(user_id)
except RedisError:
    // Fallback: In-memory rate limiting per service instance
    // Not perfect (can be bypassed with load balancer), but better than nothing
    
    if local_rate_limiter.check(user_id):
        allow request (with warning log)
    else:
        reject request
        
    // Alert ops team: "Redis down, rate limiting degraded"
```

**Best practice for production:**
- âœ… Fail open for read operations (redirects)
- âœ… Fail closed for write operations (create URLs) if abuse is critical concern
- âœ… Always log and alert when falling back

---

#### **Rate Limiting Response Format:**

**When request is allowed:**
```
HTTP/1.1 201 Created
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 87
X-RateLimit-Reset: 1638363600

{
  "shortUrl": "https://tiny.url/aBc123",
  "longUrl": "https://google.com"
}
```

**When request is rejected:**
```
HTTP/1.1 429 Too Many Requests
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 0
X-RateLimit-Reset: 1638363600
Retry-After: 3600

{
  "error": "RATE_LIMIT_EXCEEDED",
  "message": "You have exceeded your daily limit of 100 URLs. Limit resets in 1 hour.",
  "retry_after_seconds": 3600
}
```

**Headers explanation:**
- `X-RateLimit-Limit`: Total allowed in window
- `X-RateLimit-Remaining`: How many left
- `X-RateLimit-Reset`: Unix timestamp when limit resets
- `Retry-After`: Seconds to wait (standard HTTP header)

---

#### **Advanced: Sliding Window vs Fixed Window**

**Fixed Window Problem:**

```
Time:   00:00:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 00:01:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 00:02:00
Limit:  10 requests/minute

User:   00:00:50: Makes 10 requests âœ“ (Window 1)
        00:01:05: Makes 10 requests âœ“ (Window 2)

Result: 20 requests in 15 seconds! Burst attack possible! âŒ
```

**Sliding Window Solution:**

```
Track requests in last 60 seconds (sliding), not per-minute windows

Redis data:
Key: "ratelimit:user:12345:timestamps"
Value: [
  1638360050,  // Timestamp 1
  1638360051,  // Timestamp 2
  1638360052,  // Timestamp 3
  ...
]

Check:
  1. Remove timestamps older than 60 seconds
  2. Count remaining timestamps
  3. If < limit, allow and add new timestamp
```

**Trade-off:**
- âœ… More accurate (prevents burst attacks)
- âŒ More memory (store individual timestamps)
- âŒ More complex

**For URL shortener:** Token bucket is sufficient (simpler, less memory)

---

### ğŸ“Š Key Takeaways

1. **Centralized state is critical:** Use Redis to share rate limit counters across all service instances

2. **Rate limit EARLY:** Check before expensive operations (DB, validation, etc.)

3. **Fail gracefully:** When Redis is down, degrade to local rate limiting (imperfect but better than crash)

4. **Return helpful headers:** `X-RateLimit-*` and `Retry-After` help clients behave well

5. **Layer your defenses:** CDN â†’ Load Balancer â†’ Application â†’ Database

6. **Use Lua scripts:** Atomic Redis operations prevent race conditions

---

### ğŸ”— Further Reading

- **Redis Lua scripting:** Atomic rate limiting patterns
- **IETF Draft:** "RateLimit Header Fields for HTTP"
- **Kong/NGINX:** API gateway rate limiting plugins

---

### âœï¸ Practice Exercise

**Scenario:** You notice some users are bypassing rate limits by:
1. Using multiple API keys
2. Using VPNs to change IPs
3. Distributing requests across time zones

**Question:** How would you detect and prevent this abuse?

<details>
<summary>Click to see answer</summary>

**Detection strategies:**

1. **Behavioral analysis:**
   - Track patterns: Same long URLs from "different" users
   - Flag suspicious: 10 users creating identical URLs within 1 hour

2. **Device fingerprinting:**
   - Browser fingerprint (canvas, WebGL, fonts)
   - Combine with IP for more accurate identification

3. **CAPTCHA after threshold:**
   - After 3 URLs: No CAPTCHA
   - After 5 URLs: Show CAPTCHA
   - After 10 URLs: Require phone verification

4. **Honeypot URLs:**
   - Hidden form fields (bots fill them)
   - Track users who submit honeypot data

**Prevention:**

1. **Require email verification** for higher limits
2. **Phone verification** for custom URLs
3. **Credit card** (even $0 charge) for premium tier
4. **Manual review** for high-volume users

**Redis tracking:**
```
Key: "abuse:url_hash:abc123def456"
Value: List of user_ids who created this URL
Alert if: Same URL created by 5+ users in 1 hour
```

</details>

---

## **1.3 Async Processing with Message Queues**

### ğŸ¤” Interview Question/Context

**Interviewer:** "You mentioned an 'Analytics DB' in your diagram. How does click data get from your main service to the analytics database? Does the service write to both the main DB and analytics DB synchronously?"

### âŒ Your Initial Answer

In your diagram, you showed:
```
Service â†’ Master DB
Service â†’ Analytics DB
```

But you didn't explain:
- **When/how** data flows to Analytics DB
- Whether it's **synchronous** (blocking) or **asynchronous** (non-blocking)
- What happens if Analytics DB is slow or down

**The problem:** If the service writes to Analytics DB synchronously, a slow analytics query blocks the user's redirect!

```
User clicks link â†’ Service records analytics â†’ Analytics DB slow (500ms) 
â†’ User waits 500ms â†’ Bad UX! âŒ
```

---

### âœ… Complete Answer

#### **Why Async Processing is Critical:**

**Synchronous approach (BAD):**

```
User Request â†’ Service â†’ Write to Main DB âœ“ (50ms)
                      â†’ Write to Analytics DB âœ“ (500ms)
                      â†’ Return response
                      
Total latency: 550ms âŒ
```

**If Analytics DB is down:**
```
User Request â†’ Service â†’ Write to Main DB âœ“
                      â†’ Write to Analytics DB âœ— (timeout)
                      â†’ Return 500 error to user âŒ
```

**Async approach (GOOD):**

```
User Request â†’ Service â†’ Write to Main DB âœ“ (50ms)
                      â†’ Publish event to Queue âœ“ (2ms)
                      â†’ Return response immediately
                      
Total latency: 52ms âœ“

Background Worker â†’ Read from Queue
                 â†’ Write to Analytics DB
                 â†’ User doesn't wait!
```

**Key principle:** User-facing operations must be **fast** and **reliable**. Analytics is important but **not critical** for user experience.

---

#### **Message Queue Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ASYNC EVENT FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[User Action]
     â†“
[Service] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“                                   â”‚
[Main DB]                                â”‚ Publish event
     â†“                                   â”‚ (non-blocking)
[Return 302]                             â†“
     â†“                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
User gets response fast!        â”‚  Kafka Cluster â”‚
(50ms)                          â”‚  (Event Queue) â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â†“                â†“                â†“
                [Consumer Group: Analytics Workers]
                [Worker 1]      [Worker 2]      [Worker 3]
                        â†“                â†“                â†“
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â†“
                                [Analytics DB]
                                (ClickHouse)
                                         â†“
                                [Dashboard/Reports]
```

---

#### **Kafka vs RabbitMQ vs Amazon SQS:**

| Feature | Kafka âœ… | RabbitMQ | Amazon SQS |
|---------|---------|-----------|------------|
| **Throughput** | Millions/sec | Thousands/sec | Thousands/sec |
| **Durability** | Disk persistence | Memory + disk | Fully managed |
| **Ordering** | Partition-level | Queue-level | FIFO queues |
| **Replay** | Yes (retain logs) | No | Limited |
| **Scalability** | Horizontal | Vertical | Auto-scaling |
| **Best for** | High-volume analytics | Task queues | Serverless/AWS |

**For URL shortener:** **Kafka is best choice**

**Why?**
- âœ… Handle 4,000 clicks/sec easily (can scale to millions)
- âœ… Persistent logs (can replay if analytics processing fails)
- âœ… Multiple consumers (analytics, fraud detection, ML training)
- âœ… Partition by short_url_id (parallel processing)

---

### ğŸ¯ URL Shortener Context

#### **Event Types:**

**1. URL Created Event:**
```json
{
  "event_type": "url_created",
  "event_id": "evt_123456789",
  "timestamp": "2025-11-29T10:30:00Z",
  "data": {
    "short_url_id": 123456789,
    "short_code": "aBc123",
    "long_url": "https://google.com",
    "user_id": 456,
    "user_ip": "192.168.1.1",
    "created_via": "web_ui"
  }
}
```

**2. URL Clicked Event:**
```json
{
  "event_type": "url_clicked",
  "event_id": "evt_987654321",
  "timestamp": "2025-11-29T10:35:00Z",
  "data": {
    "short_url_id": 123456789,
    "short_code": "aBc123",
    "long_url": "https://google.com",
    "user_ip": "203.45.67.89",
    "user_agent": "Mozilla/5.0...",
    "referer": "https://twitter.com",
    "country": "US",
    "city": "San Francisco"
  }
}
```

---

#### **Kafka Topic Structure:**

```
Topic: url-events
  Partitions: 10 (for parallel processing)
  Replication: 3 (for durability)
  Retention: 7 days (can replay last week's events)

Partitioning strategy:
  Key: short_url_id
  Why: All events for same URL go to same partition
       â†’ Maintains ordering per URL
       â†’ Worker can process in sequence
```

**Example:**
```
URL aBc123 (ID: 100) â†’ Partition 0
URL xYz789 (ID: 200) â†’ Partition 0
URL mNo456 (ID: 300) â†’ Partition 1

All events for aBc123 are ordered in Partition 0
```

---

#### **Service Publishing Logic:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          REDIRECT WITH ASYNC ANALYTICS                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GET /aBc123
     â†“
[Service]
     â†“
Decode â†’ ID: 123456789
     â†“
[Check Cache/DB]
     â†“
Got: https://google.com
     â†“
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PUBLISH EVENT TO KAFKA (Non-blocking!)                 â”‚
â”‚                                                          â”‚
â”‚  Event: {                                                â”‚
â”‚    type: "url_clicked",                                  â”‚
â”‚    short_url_id: 123456789,                              â”‚
â”‚    user_ip: "...",                                       â”‚
â”‚    timestamp: now()                                      â”‚
â”‚  }                                                       â”‚
â”‚                                                          â”‚
â”‚  Kafka: kafka.publish("url-events", event)              â”‚
â”‚  - Async operation (fire-and-forget)                    â”‚
â”‚  - Returns immediately (doesn't wait)                   â”‚
â”‚  - If Kafka slow/down: log warning, continue            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Return 302 https://google.com
     â†“
User redirected! (Total: 50ms)


MEANWHILE (in background):
     
Kafka â†’ Consumer Group (Analytics Workers)
     â†“
[Worker 1] processes event
     â†“
Enrich event:
  - GeoIP lookup: IP â†’ Country/City
  - User-Agent parse: Device/Browser/OS
     â†“
[Batch insert to ClickHouse]
  - Collect 10,000 events
  - Bulk insert (efficient!)
     â†“
[Analytics DB updated]
```

---

#### **Consumer Worker Logic:**

**What the analytics worker does:**

```
FUNCTION process_click_event(event):
    
    // 1. Enrich event with additional data
    geo_data = geoip_lookup(event.user_ip)
    event.country = geo_data.country
    event.city = geo_data.city
    
    device_data = parse_user_agent(event.user_agent)
    event.device_type = device_data.device  // "mobile", "desktop"
    event.browser = device_data.browser      // "Chrome", "Safari"
    event.os = device_data.os                // "iOS", "Windows"
    
    // 2. Add to batch
    batch.add(event)
    
    // 3. When batch reaches 10,000 events or 30 seconds elapsed
    IF batch.size >= 10000 OR time_elapsed > 30:
        clickhouse.bulk_insert(batch)
        batch.clear()
```

**Why batching?**
- âœ… ClickHouse performs best with bulk inserts
- âœ… 10,000 events in 1 insert vs 10,000 individual inserts
- âœ… Reduces DB load by 1000x

---

#### **Handling Kafka Failures:**

**Scenario 1: Kafka is temporarily down**

```
Service tries to publish event â†’ Kafka unavailable
     â†“
Options:
  A) Retry with timeout (5 attempts, 100ms each)
  B) Log event to local file (backup)
  C) Continue anyway (user experience not affected)
  
Chosen: B + C
  - Log to file: /var/log/events/2025-11-29.log
  - Background job: Replay from file when Kafka recovers
  - User: Gets redirect immediately (doesn't know Kafka failed)
```

**Scenario 2: Consumer worker crashes**

```
Kafka retains events for 7 days
     â†“
Worker crashes mid-processing
     â†“
Kafka: Event still in queue (not committed)
     â†“
New worker starts
     â†“
Kafka: Sends same event again
     â†“
Worker: Processes event (idempotent operation)
     â†“
Success! No data lost.
```

**Key: Idempotent processing**
```
ClickHouse: INSERT with deduplication
  - Use event_id as unique key
  - Duplicate inserts ignored
  - Same event processed twice = only stored once
```

---

#### **Multiple Consumers for Different Purposes:**

```
                    [Kafka Topic: url-events]
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                    â†“                    â†“
[Consumer Group 1]   [Consumer Group 2]   [Consumer Group 3]
Analytics Workers    Fraud Detection      ML Training
        â†“                    â†“                    â†“
[ClickHouse]          [Flag suspicious]   [Click prediction]
                      [Block if abuse]    [Recommendation]
```

**Each consumer group processes ALL events independently!**

**Use cases:**

1. **Analytics Workers:** 
   - Store in ClickHouse
   - Generate reports/dashboards

2. **Fraud Detection Workers:**
   - Detect bot traffic (100 clicks in 1 second)
   - Flag malicious URLs (many users report it)
   - Block abusive IPs

3. **ML Training Workers:**
   - Train click prediction models
   - Recommendation engine (similar URLs)
   - Spam detection

**Benefit:** Add new consumers without changing existing system!

---

#### **Event Ordering Guarantees:**

**Problem:** What if events arrive out of order?

```
User clicks URL:
  Event 1: Click at 10:00:00 â†’ Network delay â†’ Arrives at 10:00:05
  Event 2: Click at 10:00:02 â†’ Fast network â†’ Arrives at 10:00:03

Worker receives: Event 2 before Event 1! âŒ
```

**Kafka solution: Partition-level ordering**

```
All events for same short_url_id go to same partition
Partition guarantees FIFO order
Worker processes events in order within partition

Result: Events for URL aBc123 are always ordered âœ“
        Events for different URLs may be out of order (OK!)
```

**Do we need strict ordering?**
- For analytics: **NO** (don't care if Event A processed before Event B)
- For state machines: **YES** (order matters)

**URL shortener:** Analytics don't need strict ordering, so we're fine!

---

#### **Monitoring Async Processing:**

**Key metrics to track:**

```
1. Kafka Producer (Service side):
   - events_published_per_second
   - publish_errors_per_second
   - publish_latency_p95
   
   Alert if:
     - publish_errors > 10/sec (Kafka having issues)
     - publish_latency > 100ms (Kafka overloaded)

2. Kafka Broker:
   - consumer_lag (events waiting to be processed)
   - partition_count
   - disk_usage
   
   Alert if:
     - consumer_lag > 100,000 (workers too slow)
     - disk_usage > 80% (need more storage)

3. Consumer Workers:
   - events_processed_per_second
   - processing_errors_per_second
   - clickhouse_insert_latency
   
   Alert if:
     - processing_errors > 5/sec (workers buggy)
     - events_processed < 1000/sec (workers too slow, scale up!)

4. Analytics DB (ClickHouse):
   - rows_inserted_per_second
   - query_latency_p95
   
   Alert if:
     - query_latency > 5 seconds (dashboard slow)
```

---

### ğŸ“Š Key Takeaways

1. **Never block user requests with analytics:** Use async processing (Kafka) so users get instant responses

2. **Kafka over RabbitMQ:** For high-throughput analytics (millions of events/day)

3. **Batch processing:** Workers collect 10K events â†’ bulk insert (efficient!)

4. **Multiple consumers:** Same events processed by analytics, fraud detection, ML training

5. **Idempotent operations:** Same event processed twice = no problem (use event_id)

6. **Monitor consumer lag:** If lag grows, scale up workers

7. **Graceful degradation:** If Kafka down, log to file, replay later

---

### ğŸ”— Further Reading

- **Kafka documentation:** Partitions, Consumer Groups, Offsets
- **"Designing Data-Intensive Applications":** Chapter 11 (Stream Processing)
- **LinkedIn Engineering Blog:** How they use Kafka at scale

---

### âœï¸ Practice Exercise

**Scenario:** Your consumer lag keeps growing. Workers are processing 3,000 events/sec, but Kafka is receiving 5,000 events/sec.

**Questions:**
1. What's the immediate problem?
2. How would you solve it short-term?
3. How would you prevent it long-term?

<details>
<summary>Click to see answer</summary>

**1. Immediate problem:**
- Consumer lag growing at 2,000 events/sec
- In 1 hour: 7.2 million events backlogged
- Analytics data delayed by hours
- If continues: Kafka disk fills up â†’ system crash

**2. Short-term solutions:**

**Option A: Scale up workers (fastest)**
```
Current: 3 workers processing 3,000 events/sec
Add: 2 more workers
New total: 5 workers â†’ 5,000 events/sec âœ“
Lag stops growing, starts decreasing
```

**Option B: Increase batch size**
```
Current: Batch of 10,000 events â†’ ClickHouse insert takes 500ms
New: Batch of 50,000 events â†’ Insert takes 1 second
Result: 5x fewer inserts â†’ 5x faster processing
```

**Option C: Optimize ClickHouse inserts**
```
- Use more efficient data types
- Remove unnecessary indexes during insert
- Use multiple ClickHouse nodes (sharding)
```

**3. Long-term prevention:**

**A. Auto-scaling:**
```
Monitor: consumer_lag metric
Rule: If lag > 50,000 for 5 minutes
Action: Add 2 more worker instances
Result: Automatic scaling during traffic spikes
```

**B. Capacity planning:**
```
Expected traffic: 4,000 events/sec average
Peak traffic: 10,000 events/sec (5x surge during viral link)
Worker capacity: 1,000 events/sec per worker
Required workers: 10,000 / 1,000 = 10 workers minimum
Add 50% buffer: 15 workers total
```

**C. Alternative: Downsample during peak**
```
If lag > 100,000:
  - Sample 10% of click events (still statistically significant)
  - Process all "url_created" events (never drop these)
  - Resume 100% processing when lag recovers
```

</details>

---

## **1.4 Service Component Clarity**

### ğŸ¤” Interview Question/Context

**Interviewer:** "I see a box labeled 'Long URL Service' in your architecture receiving the 302 redirect. Can you explain what this component does and why it's separate from your main service?"

### âŒ Your Initial Answer

In your diagram, you drew:

```
Client â†’ Service â†’ 302 Redirect â†’ Long URL Service
```

This created confusion because:
- **It looked like a separate microservice** for handling long URLs
- **Purpose was unclear** - what does this service actually do?
- **302 redirect endpoint** - the client receives the redirect and goes directly to the destination, no intermediate service needed

**The issue:** Unclear service boundaries and confusion about how HTTP redirects work.

---

### âœ… Complete Answer

#### **How HTTP 302 Redirect Actually Works:**

**User's browser behavior:**

```
Step 1: User clicks https://tiny.url/aBc123
        â†“
Step 2: Browser sends: GET /aBc123
        â†“
Step 3: Service responds:
        HTTP/1.1 302 Found
        Location: https://google.com
        â†“
Step 4: Browser AUTOMATICALLY makes new request:
        GET https://google.com
        â†“
Step 5: Google.com responds with its homepage
        â†“
User sees: Google homepage

NO "Long URL Service" needed!
Browser handles redirect automatically.
```

**Key insight:** Once service returns 302, the client (browser) takes over. Service is done!

---

#### **Correct Service Architecture:**

**Option 1: Monolithic Service (Recommended for URL Shortener)** âœ…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      URL Shortener Service               â”‚
â”‚                                          â”‚
â”‚  Responsibilities:                       â”‚
â”‚  1. Create short URL (POST /api/v1/urls)â”‚
â”‚  2. Redirect (GET /{shortCode})         â”‚
â”‚  3. URL validation                       â”‚
â”‚  4. Rate limiting                        â”‚
â”‚  5. Analytics event publishing           â”‚
â”‚                                          â”‚
â”‚  Components:                             â”‚
â”‚  - API handlers                          â”‚
â”‚  - Business logic                        â”‚
â”‚  - DB access layer                       â”‚
â”‚  - Cache client                          â”‚
â”‚  - Kafka producer                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why monolith for this use case?**
- âœ… Simple: Only 2 main operations (create, redirect)
- âœ… Low latency: No inter-service network calls
- âœ… Easy to deploy: Single service, one codebase
- âœ… Sufficient scale: Can handle millions of requests with horizontal scaling

**When to keep it monolithic:**
- Service is small (<10 endpoints)
- Operations are tightly coupled
- Team is small (<10 engineers)
- Latency is critical (no extra hops)

---

**Option 2: Microservices Architecture** (Only if needed at massive scale)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Gateway    â”‚  â† Single entry point
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“    â†“    â†“         â†“          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create â”‚ â”‚Redirect â”‚ â”‚ Auth   â”‚ â”‚Analytics â”‚
â”‚Service â”‚ â”‚Service  â”‚ â”‚Service â”‚ â”‚Service   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to split into microservices:**
- **Different teams:** Create URL team â‰  Analytics team
- **Different scaling needs:** Redirects need 10x more instances than create
- **Different technologies:** Redirect in Go (fast), analytics in Python (ML libraries)
- **Independent deployment:** Deploy analytics updates without touching redirect service

**For URL shortener at your scale (40 WPS, 4000 RPS):** **Monolith is better!**

---

#### **Common Microservice Anti-Patterns to Avoid:**

**Anti-Pattern 1: Over-segmentation**

```
âŒ BAD:
- URL Validation Service
- ID Generation Service  
- Database Write Service
- Cache Service
- Analytics Service

Problem: 5 network hops for 1 operation â†’ latency explosion!
Create URL: 50ms becomes 250ms (5 Ã— 50ms per hop)
```

**Anti-Pattern 2: Shared Database**

```
âŒ BAD:
Service A â”€â”
           â”œâ”€â”€â†’ [Same Database] â† Tight coupling!
Service B â”€â”˜

Problem: Services coupled via database schema
         Can't change schema without coordinating all services
```

**Anti-Pattern 3: Synchronous Chain**

```
âŒ BAD:
Client â†’ Service A â†’ Service B â†’ Service C â†’ DB
        (waits)    (waits)    (waits)

Problem: Any service failure breaks entire chain
         Latency = sum of all services
```

---

### ğŸ¯ URL Shortener Context

#### **Recommended Service Structure:**

**Single Service with Clear Internal Modules:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            URL Shortener Service (Monolith)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  API Layer (HTTP Handlers)                 â”‚         â”‚
â”‚  â”‚  - POST /api/v1/urls                       â”‚         â”‚
â”‚  â”‚  - GET /{shortCode}                        â”‚         â”‚
â”‚  â”‚  - GET /api/v1/urls/{shortCode}/info       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                   â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Business Logic Layer                      â”‚         â”‚
â”‚  â”‚  - URL validation                          â”‚         â”‚
â”‚  â”‚  - Duplicate detection                     â”‚         â”‚
â”‚  â”‚  - Snowflake ID generation                 â”‚         â”‚
â”‚  â”‚  - Base62 encoding                         â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                   â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Data Access Layer                         â”‚         â”‚
â”‚  â”‚  - Cache client (Redis)                    â”‚         â”‚
â”‚  â”‚  - Database client (PostgreSQL)            â”‚         â”‚
â”‚  â”‚  - Message queue client (Kafka)            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits of this structure:**
- âœ… **Modular code:** Easy to test each layer independently
- âœ… **Single deployment:** No microservice coordination
- âœ… **Low latency:** All logic in same process (no network calls)
- âœ… **Simple scaling:** Horizontal scaling (add more instances)

---

#### **When to Extract a Microservice:**

**Scenario 1: Analytics becomes complex**

```
BEFORE (Monolith):
Service â†’ Kafka â†’ (Analytics logic inside main service)

AFTER (Extract):
Service â†’ Kafka â†’ [Dedicated Analytics Service]
                         â†“
                  - Click aggregation
                  - Fraud detection  
                  - ML predictions
                  - Report generation
```

**Why extract?**
- Analytics team can deploy independently
- Different programming language (Python for ML)
- Different scaling (analytics needs 2 workers, main service needs 10)

---

**Scenario 2: Custom URL feature grows**

```
BEFORE:
Main Service handles:
  - Regular short URLs
  - Custom short URLs (simple)

AFTER:
Main Service: Regular short URLs
Custom URL Service: 
  - Slug availability check
  - Slug reservation system
  - Premium features (branded domains)
  - Billing integration
```

**Why extract?**
- Custom URLs have different business logic
- Separate team owns custom URL feature
- Can charge differently (premium feature)

---

#### **Service Communication Patterns:**

**Pattern 1: Synchronous (REST/gRPC)** - When you need immediate response

```
Use case: API Gateway â†’ Auth Service
         "Is this API key valid?"
         
Need immediate answer to proceed with request.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  HTTP/gRPC   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚   Auth   â”‚
â”‚    A    â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Service  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Response   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“ Wait for response before continuing
```

---

**Pattern 2: Asynchronous (Message Queue)** - When you don't need immediate response

```
Use case: URL clicked â†’ Analytics
         "Log this click event"
         
Don't need to wait for analytics to complete.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Publish   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  Consume  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ Kafka â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚Analytics â”‚
â”‚    A    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ Service  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“ Continue immediately (don't wait)
```

---

**Pattern 3: Service Mesh (Advanced)** - For complex microservice environments

```
Every service has a sidecar proxy (Envoy)
Handles:
  - Service discovery
  - Load balancing
  - Retries
  - Circuit breaking
  - Distributed tracing

Tools: Istio, Linkerd, Consul

Only needed when you have 10+ microservices!
```

---

#### **Deployment Architecture:**

**Horizontal Scaling of Monolith:**

```
                [Load Balancer]
                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“              â†“              â†“
[Service Instance 1] [Instance 2] [Instance 3]
        â”‚              â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
            [Shared: Redis, DB, Kafka]
```

**Each instance:**
- Identical code
- Stateless (no local state)
- Can handle any request
- Auto-scaled based on CPU/memory

**Scaling strategy:**
```
CPU > 70% for 5 minutes â†’ Add 2 instances
CPU < 30% for 10 minutes â†’ Remove 1 instance
Min instances: 3 (high availability)
Max instances: 20 (cost control)
```

---

#### **Database Access: Master-Slave Pattern**

**Service routing logic:**

```
FUNCTION handle_request(request):
    
    IF request.method == "GET":
        // Read operation
        db_connection = get_slave_connection()
        // Round-robin across slave replicas
        
    ELSE IF request.method in ["POST", "PUT", "DELETE"]:
        // Write operation
        db_connection = get_master_connection()
        // All writes go to master only
    
    result = db_connection.execute(query)
    
    RETURN result
```

**Why this pattern?**
- âœ… **Read scaling:** Distribute reads across multiple slaves
- âœ… **Write safety:** All writes through single master (consistency)
- âœ… **Fault tolerance:** If slave dies, route to other slaves

**Configuration:**
```
Database connections:
  Master: db-master.internal:5432 (writes only)
  Slaves: 
    - db-slave-1.internal:5432 (reads)
    - db-slave-2.internal:5432 (reads)
    - db-slave-3.internal:5432 (reads)
  
Connection pool:
  Master pool: 20 connections
  Slave pool: 50 connections (more reads!)
```

---

#### **Health Checks & Circuit Breakers:**

**Health check endpoint:**

```
GET /health

Response:
{
  "status": "healthy",
  "checks": {
    "database": "ok",
    "redis": "ok",
    "kafka": "ok"
  },
  "uptime_seconds": 86400
}
```

**Load balancer uses this:**
- Pings /health every 5 seconds
- If 3 consecutive failures â†’ remove instance from rotation
- If instance recovers â†’ add back to rotation

---

**Circuit breaker pattern:**

```
Service â†’ External API (Google Safe Browsing)

States:
1. CLOSED (normal): All requests go through
2. OPEN (failure): Block all requests (fail fast)
3. HALF-OPEN (testing): Allow 1 request to test if recovered

Transition:
CLOSED â†’ OPEN: After 5 consecutive failures
OPEN â†’ HALF-OPEN: After 60 seconds
HALF-OPEN â†’ CLOSED: If test request succeeds
HALF-OPEN â†’ OPEN: If test request fails
```

**Benefits:**
- âœ… Fail fast (don't wait for timeout)
- âœ… Give external service time to recover
- âœ… Automatic recovery testing

---

### ğŸ“Š Key Takeaways

1. **No "Long URL Service" needed:** Browser handles 302 redirects automatically

2. **Monolith is fine:** For URL shortener, single service is simpler and faster than microservices

3. **Clear layering:** API â†’ Business Logic â†’ Data Access (within monolith)

4. **Extract microservices only when:** Different teams, different scaling, different tech stacks

5. **Horizontal scaling:** Add more instances of same service (stateless design)

6. **Health checks:** Load balancer removes unhealthy instances automatically

7. **Circuit breakers:** Fail fast when dependencies are down

---

### ğŸ”— Further Reading

- **"Building Microservices" by Sam Newman:** When to split, when not to
- **Martin Fowler's blog:** "Monolith First" pattern
- **Netflix Tech Blog:** How they evolved from monolith to microservices

---

### âœï¸ Practice Exercise

**Scenario:** Your company wants to add these features:
1. QR code generation for short URLs
2. URL expiration (auto-delete after 30 days)
3. A/B testing (redirect 50% users to URL A, 50% to URL B)
4. Browser extension for creating short URLs

**Questions:**
1. Which features stay in monolith?
2. Which features become separate microservices?
3. Why?

<details>
<summary>Click to see answer</summary>

**Analysis:**

**1. QR Code Generation:**
**Decision: Keep in monolith** âœ…
**Why:**
- Simple operation (generate QR from short URL)
- Synchronous (user waits for QR code)
- Doesn't need separate scaling (not many requests)
- Can use library (qrcode.js)

**Implementation:**
```
GET /api/v1/urls/{shortCode}/qr

Returns: PNG image of QR code
```

---

**2. URL Expiration:**
**Decision: Keep in monolith** âœ…
**Why:**
- Part of core URL lifecycle
- Needs access to main database (url_mappings table)
- Simple background job (cron)

**Implementation:**
```
Cron job (runs daily):
  DELETE FROM url_mappings 
  WHERE created_at < NOW() - INTERVAL '30 days'
  AND expiration_enabled = true
```

---

**3. A/B Testing:**
**Decision: Extract to microservice** ğŸ”„
**Why:**
- Complex logic (experiment management, statistical analysis)
- Separate team (data science/experimentation team)
- Different scaling (analytics-heavy)
- Doesn't affect core redirect performance

**Architecture:**
```
User â†’ Service â†’ A/B Service: "Which variant?"
              â†’ A/B Service returns: "URL A"
              â†’ Redirect to URL A
              â†’ Log to analytics
```

**Better approach:**
```
A/B Service publishes rules to cache:
  "shortcode_abc: 50% â†’ URL A, 50% â†’ URL B"

Main service:
  Read rule from cache (fast!)
  Randomly assign user
  Redirect accordingly
```

---

**4. Browser Extension:**
**Decision: Keep in monolith (reuse API)** âœ…
**Why:**
- Browser extension is a CLIENT, not a service
- Uses existing API: POST /api/v1/urls
- No new backend service needed!

**Architecture:**
```
[Browser Extension]
      â†“ (uses existing API)
[Main Service] â†’ POST /api/v1/urls
      â†“
Create short URL as usual
```

---

**Final Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   URL Shortener Service (Monolith) â”‚
â”‚   - Create URL                     â”‚
â”‚   - Redirect                       â”‚
â”‚   - QR generation                  â”‚
â”‚   - Expiration cron                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”œâ”€â†’ [A/B Testing Service] (Microservice)
                â”‚
                â”œâ”€â†’ [Analytics Service] (Microservice)
                â”‚
                â””â”€â†’ [Fraud Detection Service] (Microservice)

Clients:
- Web UI
- Mobile App
- Browser Extension (all use same API)
```

**Principle:** Start monolith, extract only when there's clear benefit!

</details>

---

# **End of Section 1** âœ…

---

## **Section 1 Completion Checklist:**

- â˜‘ **1.1 Cache Strategy & Placement** - Understand where and how to cache
- â˜‘ **1.2 Rate Limiter Architecture** - Centralized rate limiting with Redis
- â˜‘ **1.3 Async Processing** - Kafka for analytics (non-blocking)
- â˜‘ **1.4 Service Clarity** - Monolith vs microservices decision

**Progress:** Section 1 complete! (4/32 topics = 12.5%)

---

**Ready for Section 2 (Monitoring & Observability)?** ğŸš€

Or would you like to:
- Review any topic from Section 1
- Practice exercises from Section 1
- Move to a different section

Let me know! ğŸ’ª